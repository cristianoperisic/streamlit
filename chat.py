import streamlit as st
import llm  # llm.py ë¡œì§ ê°€ì ¸ì˜¤ê¸°

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Project RM", page_icon="âš–ï¸", layout="wide")

st.title("âš–ï¸ Project RM: ë¶ˆê³µì • ì•½ê´€ ì‹¬íŒê´€")
st.markdown(
    """
ë²•ì  ê¸°ì¤€(ì•½ê´€ë²•, ë¶„ìŸí•´ê²°ê¸°ì¤€)ê³¼ ë¶„ì„í•  ì•½ê´€(ë„·í”Œë¦­ìŠ¤, ì¹´ì¹´ì˜¤í†¡ ë“±)ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.
RMì´ ìœ„í—˜í•œ ì¡°í•­ì„ ì°¾ì•„ë‚´ì–´ ë²•ì  ê·¼ê±°ì™€ í•¨ê»˜ íŒê²°í•´ ë“œë¦½ë‹ˆë‹¤.
"""
)

# ==========================================
# [ì‚¬ì´ë“œë°”] ê´€ë¦¬ììš©: ë¬¸ì„œ í•™ìŠµ
# ==========================================
with st.sidebar:
    st.header("ğŸ“š ì§€ì‹ ë² ì´ìŠ¤ ê´€ë¦¬")
    st.info("ê¶ê¸ˆí•œ ì•½ê´€ PDFë¥¼ ì—¬ê¸°ì— ì—…ë¡œë“œí•˜ì—¬ í•™ìŠµì‹œí‚¤ì„¸ìš”.")

    uploaded_files = st.file_uploader(
        "PDF íŒŒì¼ ì—…ë¡œë“œ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)", type=["pdf"], accept_multiple_files=True
    )

    # ì—…ë¡œë“œ ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œë§Œ ë™ì‘í•˜ê²Œ í•˜ë ¤ë©´ ë²„íŠ¼ ì¶”ê°€ ê°€ëŠ¥ (ì—¬ê¸°ì„  ìë™ ì²˜ë¦¬)
    if uploaded_files:
        if st.button("ì§€ì‹ ë² ì´ìŠ¤ì— ì—…ë¡œë“œ ë° í•™ìŠµ ì‹œì‘"):
            with st.spinner("ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  Pineconeì— ì €ì¥ ì¤‘ì…ë‹ˆë‹¤..."):
                success, message = llm.embed_documents(uploaded_files)
                if success:
                    st.success(f"âœ… í•™ìŠµ ì™„ë£Œ! {message}")
                else:
                    st.error(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {message}")

    st.divider()
    st.caption("Powered by LangChain & Pinecone")

# ==========================================
# [ë©”ì¸] ì‚¬ìš©ììš©: ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
# ==========================================

# 1. ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {
            "role": "assistant",
            "content": "ì•ˆë…•í•˜ì„¸ìš”! RMì…ë‹ˆë‹¤. ì•½ê´€ íŒŒì¼ë“¤ì„ í•™ìŠµì‹œí‚¤ì…¨ë‚˜ìš”? ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë´ ì£¼ì„¸ìš”.",
        }
    ]

# 2. ì´ì „ ëŒ€í™” ì¶œë ¥
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

# 3. ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ë„·í”Œë¦­ìŠ¤ í™˜ë¶ˆ ê·œì •ì€ ê³µì •í•´?)"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ì €ì¥
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.write(user_input)

    # 4. ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ë²•ë ¹ê³¼ ì•½ê´€ì„ ëŒ€ì¡°í•˜ì—¬ íŒê²° ì¤‘..."):
            try:
                # RAG ì²´ì¸ ê°€ì ¸ì˜¤ê¸°
                qa_chain = llm.get_rag_chain()

                # ë‹µë³€ ìš”ì²­
                response = qa_chain.invoke({"query": user_input})
                result_text = response["result"]

                st.write(result_text)

                # ë‹µë³€ ì €ì¥
                st.session_state.messages.append(
                    {"role": "assistant", "content": result_text}
                )

            except Exception as e:
                st.error(
                    f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\nì˜¤ë¥˜ ë‚´ìš©: {e}"
                )
